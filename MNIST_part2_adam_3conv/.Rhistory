#})
with(tf$name_scope("conv3_activation"), {
W_conv3 <- weight_variable(shape = shape(5L, 5L, 8L, 8L))
b_conv3 <- bias_variable(shape = shape(8L))
h_conv3 <- tf$nn$relu(conv2d(h_conv2, W_conv3) + b_conv3)
tf$summary$histogram("activation_conv3", h_conv3)
})
###Densely Connected Layer
with(tf$name_scope("flatten_pool2_activation"), {
W_fc1 <- weight_variable(shape(28L * 28L * 8L, 6272L))
b_fc1 <- bias_variable(shape(6272L))
h_fc1_flat <- tf$reshape(h_conv3, shape(-1L, 28L * 28L * 8L))
tf$summary$histogram("flatten_pool2_activation", h_fc1_flat)
})
with(tf$name_scope("fully_connected_1_activation"), {
h_fc1 <- tf$nn$relu(tf$matmul(h_fc1_flat, W_fc1) + b_fc1)
tf$summary$histogram("fully_connected_1_activation", h_fc1)
})
### Drop Out
with(tf$name_scope("fully_connected_1_activation_after_dropout"), {
keep_prob <- tf$placeholder(tf$float32)
h_fc1_drop <- tf$nn$dropout(h_fc1, keep_prob)
tf$summary$histogram("fully_connected_1_activation_after_dropout", h_fc1_drop)
})
### Readout Layer
with(tf$name_scope("prediction_after_softmax"), {
W_fc2 <- weight_variable(shape(6272L, 10L))
b_fc2 <- bias_variable(shape(10L))
y_conv <- tf$nn$softmax(tf$matmul(h_fc1_drop, W_fc2) + b_fc2)
tf$summary$histogram("prediction_after_softmax", y_conv)
})
#### derive loss minimization function and accuracy measure
with(tf$name_scope("cross_entropy"), {
cross_entropy <- tf$reduce_mean(-tf$reduce_sum(y_ * tf$log(y_conv), reduction_indices=1L))
tf$summary$scalar("cross entropy", cross_entropy)
})
with(tf$name_scope("train"), {
train_step <- tf$train$AdamOptimizer(learning_rate)$minimize(cross_entropy)
})
with(tf$name_scope("accuracy"), {
correct_prediction <- tf$equal(tf$argmax(y_conv, 1L), tf$argmax(y_, 1L))
accuracy <- tf$reduce_mean(tf$cast(correct_prediction, tf$float32))
tf$summary$scalar("accuracy", accuracy)
})
# Merge all the summaries and write them out to /tmp/mnist_logs (by default)
merged <- tf$summary$merge_all()
train_writer <- tf$summary$FileWriter(file.path(summaries_dir, "train"),
sess$graph)
test_writer <- tf$summary$FileWriter(file.path(summaries_dir, "test"))
### initialize all of the variables
sess$run(tf$global_variables_initializer())
### Train and Evaluate the Model
for (i in 1:max_steps) {
if (i %% 100 == 0) { # Record summaries and test set accuracy
result <- sess$run(list(merged,accuracy),feed_dict = feed_dict(FALSE))
summary <- result[[1]]
test_accuracy <-  result[[2]]
test_writer$add_summary(summary, i)
cat(sprintf("step %d, Test accuracy %g\n", i, test_accuracy))
}
else { # Record train set summaries and train
result <- sess$run(list(merged,train_step),feed_dict = feed_dict(TRUE))
summary <- result[[1]]
train_writer$add_summary(summary, i)
}
}
knitr::opts_chunk$set(echo = TRUE)
#### initialization
library(tensorflow)
set.seed(42)
max_steps <- 10000
batch_size <- 100
learning_rate <- 0.001
training_dropout_rate <- 0.4
summaries_dir <- '/Users/markberman/cs498_AML/assignment9/MNIST_part2_adam_3conv/mnist_logs'
# Make a TensorFlow feed_dict: maps data onto Tensor placeholders.
feed_dict <- function(train) {
if (train){
batch <- mnist$train$next_batch(50L)
xs <- batch[[1]]
ys <- batch[[2]]
k <- training_dropout_rate
} else {
xs <- mnist$test$images
ys <- mnist$test$labels
k <- 1.0
}
dict(x = xs,
y_ = ys,
keep_prob = k)
}
# initialize summaries_dir (remove existing if necessary)
if (tf$gfile$Exists(summaries_dir))
tf$gfile$DeleteRecursively(summaries_dir)
tf$gfile$MakeDirs(summaries_dir)
#read the data
library(tensorflow)
input_dataset <- tf$examples$tutorials$mnist$input_data
mnist <- input_dataset$read_data_sets("MNIST-data", one_hot = TRUE)
### start an interactive tensor flow session
sess <- tf$InteractiveSession()
### add place holders and variables and initialize variables
x <- tf$placeholder(tf$float32, shape(NULL, 784L),name="images")
y_ <- tf$placeholder(tf$float32, shape(NULL, 10L), name = "training_labels")
#weight initialization functions
weight_variable <- function(shape) {
initial <- tf$truncated_normal(shape, stddev=0.1)
tf$Variable(initial)
}
bias_variable <- function(shape) {
initial <- tf$constant(0.1, shape=shape)
tf$Variable(initial)
}
##Convolution and Pooling
conv2d <- function(x, W) {
tf$nn$conv2d(x, W, strides=c(1L, 1L, 1L, 1L), padding='SAME')
}
max_pool_2x2 <- function(x) {
tf$nn$max_pool(
x,
ksize=c(1L, 2L, 2L, 1L),
strides=c(1L, 2L, 2L, 1L),
padding='SAME')
}
##### first convolution layer
with(tf$name_scope("conv1_activation"), {
W_conv1 <- weight_variable(shape(5L, 5L, 1L, 8L))
b_conv1 <- bias_variable(shape(8L))
x_image <- tf$reshape(x, shape(-1L, 28L, 28L, 1L))
h_conv1 <- tf$nn$relu(conv2d(x_image, W_conv1) + b_conv1)
#tf$summary$histogram("activation_conv1", h_conv1)
})
#with(tf$name_scope("pool_activation"), {
#  h_pool1 <- max_pool_2x2(h_conv1)
#  tf$summary$histogram("activation_pool1", h_pool1)
#})
##### second convolutional layer
with(tf$name_scope("conv2_activation"), {
W_conv2 <- weight_variable(shape = shape(5L, 5L, 8L, 8L))
b_conv2 <- bias_variable(shape = shape(8L))
h_conv2 <- tf$nn$relu(conv2d(h_conv1, W_conv2) + b_conv2)
#tf$summary$histogram("activation_conv2", h_conv2)
})
#with(tf$name_scope("poo2_activation"), {
#  h_pool2 <- max_pool_2x2(h_conv2)
#  tf$summary$histogram("activation_pool2", h_pool2)
#})
with(tf$name_scope("conv3_activation"), {
W_conv3 <- weight_variable(shape = shape(5L, 5L, 8L, 8L))
b_conv3 <- bias_variable(shape = shape(8L))
h_conv3 <- tf$nn$relu(conv2d(h_conv2, W_conv3) + b_conv3)
#tf$summary$histogram("activation_conv3", h_conv3)
})
###Densely Connected Layer
with(tf$name_scope("flatten_pool2_activation"), {
W_fc1 <- weight_variable(shape(28L * 28L * 8L, 6272L))
b_fc1 <- bias_variable(shape(6272L))
h_fc1_flat <- tf$reshape(h_conv3, shape(-1L, 28L * 28L * 8L))
#tf$summary$histogram("flatten_pool2_activation", h_fc1_flat)
})
with(tf$name_scope("fully_connected_1_activation"), {
h_fc1 <- tf$nn$relu(tf$matmul(h_fc1_flat, W_fc1) + b_fc1)
#tf$summary$histogram("fully_connected_1_activation", h_fc1)
})
### Drop Out
with(tf$name_scope("fully_connected_1_activation_after_dropout"), {
keep_prob <- tf$placeholder(tf$float32)
h_fc1_drop <- tf$nn$dropout(h_fc1, keep_prob)
#tf$summary$histogram("fully_connected_1_activation_after_dropout", h_fc1_drop)
})
### Readout Layer
with(tf$name_scope("prediction_after_softmax"), {
W_fc2 <- weight_variable(shape(6272L, 10L))
b_fc2 <- bias_variable(shape(10L))
y_conv <- tf$nn$softmax(tf$matmul(h_fc1_drop, W_fc2) + b_fc2)
#tf$summary$histogram("prediction_after_softmax", y_conv)
})
#### derive loss minimization function and accuracy measure
with(tf$name_scope("cross_entropy"), {
cross_entropy <- tf$reduce_mean(-tf$reduce_sum(y_ * tf$log(y_conv), reduction_indices=1L))
tf$summary$scalar("cross entropy", cross_entropy)
})
with(tf$name_scope("train"), {
train_step <- tf$train$AdamOptimizer(learning_rate)$minimize(cross_entropy)
})
with(tf$name_scope("accuracy"), {
correct_prediction <- tf$equal(tf$argmax(y_conv, 1L), tf$argmax(y_, 1L))
accuracy <- tf$reduce_mean(tf$cast(correct_prediction, tf$float32))
tf$summary$scalar("accuracy", accuracy)
})
# Merge all the summaries and write them out to /tmp/mnist_logs (by default)
merged <- tf$summary$merge_all()
train_writer <- tf$summary$FileWriter(file.path(summaries_dir, "train"),
sess$graph)
test_writer <- tf$summary$FileWriter(file.path(summaries_dir, "test"))
### initialize all of the variables
sess$run(tf$global_variables_initializer())
### Train and Evaluate the Model
for (i in 1:max_steps) {
if (i %% 100 == 0) { # Record summaries and test set accuracy
result <- sess$run(list(merged,accuracy),feed_dict = feed_dict(FALSE))
summary <- result[[1]]
test_accuracy <-  result[[2]]
test_writer$add_summary(summary, i)
cat(sprintf("step %d, Test accuracy %g\n", i, test_accuracy))
}
else { # Record train set summaries and train
result <- sess$run(list(merged,train_step),feed_dict = feed_dict(TRUE))
summary <- result[[1]]
train_writer$add_summary(summary, i)
}
}
knitr::opts_chunk$set(echo = TRUE)
#### initialization
library(tensorflow)
set.seed(42)
max_steps <- 10000
batch_size <- 50
learning_rate <- 0.0001
training_dropout_rate <- 0.7
summaries_dir <- '/Users/markberman/cs498_AML/assignment9/MNIST_part2_adam_3conv/mnist_logs'
# Make a TensorFlow feed_dict: maps data onto Tensor placeholders.
feed_dict <- function(train) {
if (train){
batch <- mnist$train$next_batch(50L)
xs <- batch[[1]]
ys <- batch[[2]]
k <- training_dropout_rate
} else {
xs <- mnist$test$images
ys <- mnist$test$labels
k <- 1.0
}
dict(x = xs,
y_ = ys,
keep_prob = k)
}
# initialize summaries_dir (remove existing if necessary)
if (tf$gfile$Exists(summaries_dir))
tf$gfile$DeleteRecursively(summaries_dir)
tf$gfile$MakeDirs(summaries_dir)
#read the data
library(tensorflow)
input_dataset <- tf$examples$tutorials$mnist$input_data
mnist <- input_dataset$read_data_sets("MNIST-data", one_hot = TRUE)
### start an interactive tensor flow session
sess <- tf$InteractiveSession()
### add place holders and variables and initialize variables
x <- tf$placeholder(tf$float32, shape(NULL, 784L),name="images")
y_ <- tf$placeholder(tf$float32, shape(NULL, 10L), name = "training_labels")
#weight initialization functions
weight_variable <- function(shape) {
initial <- tf$truncated_normal(shape, stddev=0.1)
tf$Variable(initial)
}
bias_variable <- function(shape) {
initial <- tf$constant(0.1, shape=shape)
tf$Variable(initial)
}
##Convolution and Pooling
conv2d <- function(x, W) {
tf$nn$conv2d(x, W, strides=c(1L, 1L, 1L, 1L), padding='SAME')
}
max_pool_2x2 <- function(x) {
tf$nn$max_pool(
x,
ksize=c(1L, 2L, 2L, 1L),
strides=c(1L, 2L, 2L, 1L),
padding='SAME')
}
##### first convolution layer
with(tf$name_scope("conv1_activation"), {
W_conv1 <- weight_variable(shape(5L, 5L, 1L, 8L))
b_conv1 <- bias_variable(shape(8L))
x_image <- tf$reshape(x, shape(-1L, 28L, 28L, 1L))
h_conv1 <- tf$nn$relu(conv2d(x_image, W_conv1) + b_conv1)
#tf$summary$histogram("activation_conv1", h_conv1)
})
#with(tf$name_scope("pool_activation"), {
#  h_pool1 <- max_pool_2x2(h_conv1)
#  tf$summary$histogram("activation_pool1", h_pool1)
#})
##### second convolutional layer
with(tf$name_scope("conv2_activation"), {
W_conv2 <- weight_variable(shape = shape(5L, 5L, 8L, 8L))
b_conv2 <- bias_variable(shape = shape(8L))
h_conv2 <- tf$nn$relu(conv2d(h_conv1, W_conv2) + b_conv2)
#tf$summary$histogram("activation_conv2", h_conv2)
})
#with(tf$name_scope("poo2_activation"), {
#  h_pool2 <- max_pool_2x2(h_conv2)
#  tf$summary$histogram("activation_pool2", h_pool2)
#})
with(tf$name_scope("conv3_activation"), {
W_conv3 <- weight_variable(shape = shape(5L, 5L, 8L, 8L))
b_conv3 <- bias_variable(shape = shape(8L))
h_conv3 <- tf$nn$relu(conv2d(h_conv2, W_conv3) + b_conv3)
#tf$summary$histogram("activation_conv3", h_conv3)
})
###Densely Connected Layer
with(tf$name_scope("flatten_pool2_activation"), {
W_fc1 <- weight_variable(shape(28L * 28L * 8L, 6272L))
b_fc1 <- bias_variable(shape(6272L))
h_fc1_flat <- tf$reshape(h_conv3, shape(-1L, 28L * 28L * 8L))
#tf$summary$histogram("flatten_pool2_activation", h_fc1_flat)
})
with(tf$name_scope("fully_connected_1_activation"), {
h_fc1 <- tf$nn$relu(tf$matmul(h_fc1_flat, W_fc1) + b_fc1)
#tf$summary$histogram("fully_connected_1_activation", h_fc1)
})
### Drop Out
with(tf$name_scope("fully_connected_1_activation_after_dropout"), {
keep_prob <- tf$placeholder(tf$float32)
h_fc1_drop <- tf$nn$dropout(h_fc1, keep_prob)
#tf$summary$histogram("fully_connected_1_activation_after_dropout", h_fc1_drop)
})
### Readout Layer
with(tf$name_scope("prediction_after_softmax"), {
W_fc2 <- weight_variable(shape(6272L, 10L))
b_fc2 <- bias_variable(shape(10L))
y_conv <- tf$nn$softmax(tf$matmul(h_fc1_drop, W_fc2) + b_fc2)
#tf$summary$histogram("prediction_after_softmax", y_conv)
})
#### derive loss minimization function and accuracy measure
with(tf$name_scope("cross_entropy"), {
cross_entropy <- tf$reduce_mean(-tf$reduce_sum(y_ * tf$log(y_conv), reduction_indices=1L))
tf$summary$scalar("cross entropy", cross_entropy)
})
with(tf$name_scope("train"), {
train_step <- tf$train$AdamOptimizer(learning_rate)$minimize(cross_entropy)
})
with(tf$name_scope("accuracy"), {
correct_prediction <- tf$equal(tf$argmax(y_conv, 1L), tf$argmax(y_, 1L))
accuracy <- tf$reduce_mean(tf$cast(correct_prediction, tf$float32))
tf$summary$scalar("accuracy", accuracy)
})
# Merge all the summaries and write them out to /tmp/mnist_logs (by default)
merged <- tf$summary$merge_all()
train_writer <- tf$summary$FileWriter(file.path(summaries_dir, "train"),
sess$graph)
test_writer <- tf$summary$FileWriter(file.path(summaries_dir, "test"))
### initialize all of the variables
sess$run(tf$global_variables_initializer())
### Train and Evaluate the Model
for (i in 1:max_steps) {
if (i %% 100 == 0) { # Record summaries and test set accuracy
result <- sess$run(list(merged,accuracy),feed_dict = feed_dict(FALSE))
summary <- result[[1]]
test_accuracy <-  result[[2]]
test_writer$add_summary(summary, i)
cat(sprintf("step %d, Test accuracy %g\n", i, test_accuracy))
}
else { # Record train set summaries and train
result <- sess$run(list(merged,train_step),feed_dict = feed_dict(TRUE))
summary <- result[[1]]
train_writer$add_summary(summary, i)
}
}
knitr::opts_chunk$set(echo = TRUE)
#### initialization
library(tensorflow)
set.seed(42)
max_steps <- 10000
batch_size <- 50
learning_rate <- 0.0001
training_dropout_rate <- 0.7
summaries_dir <- '/Users/markberman/cs498_AML/assignment9/MNIST_part2_adam_3conv/mnist_logs'
# Make a TensorFlow feed_dict: maps data onto Tensor placeholders.
feed_dict <- function(train) {
if (train){
batch <- mnist$train$next_batch(50L)
xs <- batch[[1]]
ys <- batch[[2]]
k <- training_dropout_rate
} else {
xs <- mnist$test$images
ys <- mnist$test$labels
k <- 1.0
}
dict(x = xs,
y_ = ys,
keep_prob = k)
}
# initialize summaries_dir (remove existing if necessary)
if (tf$gfile$Exists(summaries_dir))
tf$gfile$DeleteRecursively(summaries_dir)
tf$gfile$MakeDirs(summaries_dir)
#read the data
library(tensorflow)
input_dataset <- tf$examples$tutorials$mnist$input_data
mnist <- input_dataset$read_data_sets("MNIST-data", one_hot = TRUE)
### start an interactive tensor flow session
sess <- tf$InteractiveSession()
### add place holders and variables and initialize variables
x <- tf$placeholder(tf$float32, shape(NULL, 784L),name="images")
y_ <- tf$placeholder(tf$float32, shape(NULL, 10L), name = "training_labels")
#weight initialization functions
weight_variable <- function(shape) {
initial <- tf$truncated_normal(shape, stddev=0.1)
tf$Variable(initial)
}
bias_variable <- function(shape) {
initial <- tf$constant(0.1, shape=shape)
tf$Variable(initial)
}
##Convolution and Pooling
conv2d <- function(x, W) {
tf$nn$conv2d(x, W, strides=c(1L, 1L, 1L, 1L), padding='SAME')
}
max_pool_2x2 <- function(x) {
tf$nn$max_pool(
x,
ksize=c(1L, 2L, 2L, 1L),
strides=c(1L, 2L, 2L, 1L),
padding='SAME')
}
##### first convolution layer
with(tf$name_scope("conv1_activation"), {
W_conv1 <- weight_variable(shape(5L, 5L, 1L, 8L))
b_conv1 <- bias_variable(shape(8L))
x_image <- tf$reshape(x, shape(-1L, 28L, 28L, 1L))
h_conv1 <- tf$nn$relu(conv2d(x_image, W_conv1) + b_conv1)
#tf$summary$histogram("activation_conv1", h_conv1)
})
#with(tf$name_scope("pool_activation"), {
#  h_pool1 <- max_pool_2x2(h_conv1)
#  tf$summary$histogram("activation_pool1", h_pool1)
#})
##### second convolutional layer
with(tf$name_scope("conv2_activation"), {
W_conv2 <- weight_variable(shape = shape(5L, 5L, 8L, 8L))
b_conv2 <- bias_variable(shape = shape(8L))
h_conv2 <- tf$nn$relu(conv2d(h_conv1, W_conv2) + b_conv2)
#tf$summary$histogram("activation_conv2", h_conv2)
})
#with(tf$name_scope("poo2_activation"), {
#  h_pool2 <- max_pool_2x2(h_conv2)
#  tf$summary$histogram("activation_pool2", h_pool2)
#})
with(tf$name_scope("conv3_activation"), {
W_conv3 <- weight_variable(shape = shape(5L, 5L, 8L, 8L))
b_conv3 <- bias_variable(shape = shape(8L))
h_conv3 <- tf$nn$relu(conv2d(h_conv2, W_conv3) + b_conv3)
#tf$summary$histogram("activation_conv3", h_conv3)
})
###Densely Connected Layer
with(tf$name_scope("flatten_pool2_activation"), {
W_fc1 <- weight_variable(shape(28L * 28L * 8L, 6272L))
b_fc1 <- bias_variable(shape(6272L))
h_fc1_flat <- tf$reshape(h_conv3, shape(-1L, 28L * 28L * 8L))
#tf$summary$histogram("flatten_pool2_activation", h_fc1_flat)
})
with(tf$name_scope("fully_connected_1_activation"), {
h_fc1 <- tf$nn$relu(tf$matmul(h_fc1_flat, W_fc1) + b_fc1)
#tf$summary$histogram("fully_connected_1_activation", h_fc1)
})
### Drop Out
with(tf$name_scope("fully_connected_1_activation_after_dropout"), {
keep_prob <- tf$placeholder(tf$float32)
h_fc1_drop <- tf$nn$dropout(h_fc1, keep_prob)
#tf$summary$histogram("fully_connected_1_activation_after_dropout", h_fc1_drop)
})
### Readout Layer
with(tf$name_scope("prediction_after_softmax"), {
W_fc2 <- weight_variable(shape(6272L, 10L))
b_fc2 <- bias_variable(shape(10L))
y_conv <- tf$nn$softmax(tf$matmul(h_fc1_drop, W_fc2) + b_fc2)
#tf$summary$histogram("prediction_after_softmax", y_conv)
})
#### derive loss minimization function and accuracy measure
with(tf$name_scope("cross_entropy"), {
cross_entropy <- tf$reduce_mean(-tf$reduce_sum(y_ * tf$log(y_conv), reduction_indices=1L))
tf$summary$scalar("cross entropy", cross_entropy)
})
with(tf$name_scope("train"), {
train_step <- tf$train$AdamOptimizer(learning_rate)$minimize(cross_entropy)
})
with(tf$name_scope("accuracy"), {
correct_prediction <- tf$equal(tf$argmax(y_conv, 1L), tf$argmax(y_, 1L))
accuracy <- tf$reduce_mean(tf$cast(correct_prediction, tf$float32))
tf$summary$scalar("accuracy", accuracy)
})
# Merge all the summaries and write them out to /tmp/mnist_logs (by default)
merged <- tf$summary$merge_all()
train_writer <- tf$summary$FileWriter(file.path(summaries_dir, "train"),
sess$graph)
test_writer <- tf$summary$FileWriter(file.path(summaries_dir, "test"))
### initialize all of the variables
sess$run(tf$global_variables_initializer())
### Train and Evaluate the Model
for (i in 1:max_steps) {
if (i %% 100 == 0) { # Record summaries and test set accuracy
result <- sess$run(list(merged,accuracy),feed_dict = feed_dict(FALSE))
summary <- result[[1]]
test_accuracy <-  result[[2]]
test_writer$add_summary(summary, i)
cat(sprintf("step %d, Test accuracy %g\n", i, test_accuracy))
}
else { # Record train set summaries and train
result <- sess$run(list(merged,train_step),feed_dict = feed_dict(TRUE))
summary <- result[[1]]
train_writer$add_summary(summary, i)
}
}
test_accuracy <- accuracy$eval(feed_dict = dict(
x = mnist$test$images, y_ = mnist$test$labels, keep_prob = 1.0))
cat(sprintf("Final test accuracy %g", test_accuracy))
#### close tensorboard file writers
train_writer$close()
test_writer$close()
#### Launch Tensorboard
tensorboard(log_dir = summaries_dir)
